name: Infrastructure Destruction

on:
  workflow_dispatch:  # Manual trigger only - safe!
    inputs:
      environment:
        description: 'Environment to destroy (dev, stg, prod)'
        required: true
        type: choice
        options:
          - dev
          - stg
          - prod
        default: 'dev'
      confirm_destroy:
        description: 'Type "DESTROY" to confirm (case-sensitive)'
        required: true
        type: string

# Prevent duplicate runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  validate-destroy:
    name: Validate Destruction Request
    runs-on: ubuntu-latest
    steps:
      - name: Validate confirmation
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "âŒ Invalid confirmation. You must type 'DESTROY' to proceed."
            echo "Received: '${{ github.event.inputs.confirm_destroy }}'"
            exit 1
          fi
          echo "âœ… Destruction confirmed. Proceeding with infrastructure teardown..."

  terraform-destroy:
    name: Destroy Infrastructure (${{ github.event.inputs.environment }})
    runs-on: ubuntu-latest
    needs: validate-destroy
    env:
      ENVIRONMENT: ${{ github.event.inputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: Download state from artifacts (always try first - most recent in CI/CD)
        uses: actions/download-artifact@v4
        with:
          name: terraform-state
          path: infra/terraform/
          pattern: terraform.tfstate*
          allow-empty: true
        continue-on-error: true

      - name: Check if state exists in artifacts
        id: check-artifacts
        working-directory: infra/terraform
        run: |
          if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ]; then
            echo "âœ… State file found in artifacts (most recent from CI/CD)"
            echo "state_exists=true" >> $GITHUB_OUTPUT
            ls -lah terraform.tfstate* || true
          else
            echo "âš ï¸  State file not found in artifacts"
            echo "state_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Determine backend configuration for fallback
        id: backend-check
        if: steps.check-artifacts.outputs.state_exists == 'false'
        working-directory: infra/terraform
        run: |
          echo "ğŸ” Artifacts missing. Checking backend configuration for fallback..."
          
          # Check backend.tf to determine where state might be stored
          if grep -q 'backend "local"' backend.tf && grep -q 'path = "/mnt/terraform-state' backend.tf; then
            echo "Backend: Local on EBS volume - will fetch from server"
            echo "backend_type=ebs" >> $GITHUB_OUTPUT
          elif grep -q 'backend "s3"' backend.tf; then
            echo "Backend: S3 - state will be fetched during terraform init"
            echo "backend_type=s3" >> $GITHUB_OUTPUT
          else
            echo "Backend: Local in CI/CD runner - artifacts should have it"
            echo "backend_type=local" >> $GITHUB_OUTPUT
          fi

      - name: Fetch state from EBS volume (fallback if artifacts missing)
        if: steps.check-artifacts.outputs.state_exists == 'false' && steps.backend-check.outputs.backend_type == 'ebs'
        working-directory: infra/terraform
        run: |
          echo "ğŸ“¥ Artifacts missing. Fetching state from EBS volume on server..."
          
          # Get server IP from AWS (environment-specific)
          ENV="${{ env.ENVIRONMENT }}"
          INSTANCE_NAME="todo-app-server-${ENV}"
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=$INSTANCE_NAME" "Name=instance-state-name,Values=running,stopped,stopping,pending" \
            --query 'Reservations[*].Instances[*].[InstanceId,LaunchTime]' \
            --output text 2>/dev/null | sort -k2 -r | head -1 | awk '{print $1}' || echo "")
          
          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
            echo "âš ï¸  No instance found. Cannot fetch state from EBS volume."
            exit 0
          fi
          
          SERVER_IP=$(aws ec2 describe-instances \
            --instance-ids "$INSTANCE_ID" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text 2>/dev/null || echo "")
          
          if [ -z "$SERVER_IP" ] || [ "$SERVER_IP" == "None" ]; then
            echo "âš ï¸  Instance has no public IP. Cannot fetch state."
            exit 0
          fi
          
          echo "Found instance $INSTANCE_ID at $SERVER_IP"
          
          # Setup SSH
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H "$SERVER_IP" >> ~/.ssh/known_hosts 2>/dev/null || true
          
          # Fetch state file from EBS volume
          if scp -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa \
            "${{ secrets.TERRAFORM_SERVER_USER || 'ubuntu' }}@$SERVER_IP:/mnt/terraform-state/terraform.tfstate" \
            terraform.tfstate 2>/dev/null; then
            echo "âœ… Successfully fetched state file from EBS volume!"
            ls -lah terraform.tfstate
          else
            echo "âš ï¸  Could not fetch state file from EBS volume"
          fi
        continue-on-error: true

      - name: Verify state file exists
        id: verify-state
        working-directory: infra/terraform
        run: |
          if [ -f terraform.tfstate ] || [ -f terraform.tfstate.backup ]; then
            echo "âœ… State file found"
            echo "state_exists=true" >> $GITHUB_OUTPUT
            ls -lah terraform.tfstate* || true
          else
            echo "âš ï¸  No state file found"
            echo "This could mean:"
            echo "  - Infrastructure was never created"
            echo "  - State file was manually deleted"
            echo "  - State is in remote backend (S3) and will be fetched during init"
            echo "state_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Configure S3 Backend
        id: backend-config
        run: |
          ENV="${{ env.ENVIRONMENT }}"
          echo "Configuring S3 backend for environment: $ENV"
          
          BACKEND_BUCKET="${{ secrets.TERRAFORM_STATE_BUCKET }}"
          BACKEND_KEY="terraform-state/${ENV}/terraform.tfstate"
          BACKEND_REGION="${{ secrets.AWS_REGION || 'us-east-1' }}"
          BACKEND_DYNAMODB_TABLE="${{ secrets.TERRAFORM_STATE_LOCK_TABLE }}"
          
          if [ -z "$BACKEND_BUCKET" ] || [ -z "$BACKEND_DYNAMODB_TABLE" ]; then
            echo "âš ï¸  WARNING: Backend not configured - using local backend"
            echo "backend_configured=false" >> $GITHUB_OUTPUT
          else
            echo "backend_configured=true" >> $GITHUB_OUTPUT
            echo "backend_bucket=$BACKEND_BUCKET" >> $GITHUB_OUTPUT
            echo "backend_key=$BACKEND_KEY" >> $GITHUB_OUTPUT
            echo "backend_region=$BACKEND_REGION" >> $GITHUB_OUTPUT
            echo "backend_table=$BACKEND_DYNAMODB_TABLE" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Init (with remote backend)
        if: steps.backend-config.outputs.backend_configured == 'true'
        working-directory: infra/terraform
        run: |
          terraform init \
            -backend-config="bucket=${{ steps.backend-config.outputs.backend_bucket }}" \
            -backend-config="key=${{ steps.backend-config.outputs.backend_key }}" \
            -backend-config="region=${{ steps.backend-config.outputs.backend_region }}" \
            -backend-config="dynamodb_table=${{ steps.backend-config.outputs.backend_table }}" \
            -backend-config="encrypt=true"

      - name: Terraform Init (local backend fallback)
        if: steps.backend-config.outputs.backend_configured != 'true'
        working-directory: infra/terraform
        run: terraform init -backend=false

      - name: Use environment-specific tfvars
        id: use-tfvars
        working-directory: infra/terraform
        run: |
          ENV="${{ env.ENVIRONMENT }}"
          TFVARS_FILE="terraform.${ENV}.tfvars"
          
          if [ ! -f "$TFVARS_FILE" ]; then
            echo "âŒ Error: $TFVARS_FILE not found"
            exit 1
          fi
          
          echo "âœ… Using $TFVARS_FILE for environment: $ENV"
          echo "tfvars_file=$TFVARS_FILE" >> $GITHUB_OUTPUT

      - name: Import existing resources if state is missing
        if: steps.verify-state.outputs.state_exists == 'false'
        working-directory: infra/terraform
        run: |
          echo "âš ï¸  State file missing. Attempting to import existing resources..."
          
          ENV="${{ env.ENVIRONMENT }}"
          SG_NAME="todo-app-sg-${ENV}"
          INSTANCE_NAME="todo-app-server-${ENV}"
          
          # Import security group (environment-specific)
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[0].GroupId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            echo "Importing security group: $SG_ID"
            terraform import -var-file=${{ steps.use-tfvars.outputs.tfvars_file }} -var="key_pair_name=${{ secrets.TERRAFORM_KEY_PAIR_NAME }}" aws_security_group.todo_app "$SG_ID" || echo "Security group import failed"
          fi
          
          # Import EC2 instance (environment-specific)
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=$INSTANCE_NAME" "Name=instance-state-name,Values=running,stopped,stopping,pending" \
            --query 'Reservations[*].Instances[*].[InstanceId,LaunchTime]' \
            --output text 2>/dev/null | sort -k2 -r | head -1 | awk '{print $1}' || echo "")
          
          if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
            echo "Importing EC2 instance: $INSTANCE_ID"
            terraform import -var-file=${{ steps.use-tfvars.outputs.tfvars_file }} -var="key_pair_name=${{ secrets.TERRAFORM_KEY_PAIR_NAME }}" aws_instance.todo_app "$INSTANCE_ID" || echo "EC2 instance import failed"
          fi
          
          # Note: EBS volume imports removed - using S3 remote backend instead
          
          echo "âœ… Import complete. Proceeding with destroy..."
        continue-on-error: true

      - name: Check and detach EBS volume if needed
        working-directory: infra/terraform
        run: |
          echo "ğŸ” Checking if EBS volume attachment is in state..."
          
          # Check if volume attachment resource exists in state
          if terraform state list 2>/dev/null | grep -q "aws_volume_attachment.terraform_state"; then
            echo "âœ… Volume attachment is in state - Terraform will handle detachment"
          else
            echo "âš ï¸  Volume attachment NOT in state - checking if volume needs manual detachment"
            
            # Get instance ID from state or AWS
            INSTANCE_ID=$(terraform state show aws_instance.todo_app 2>/dev/null | grep -oP 'id\s+=\s+"\K[^"]+' | head -1 || echo "")
            
            if [ -z "$INSTANCE_ID" ]; then
              # Try to get from AWS by tag (environment-specific)
              ENV="${{ env.ENVIRONMENT }}"
              INSTANCE_NAME="todo-app-server-${ENV}"
              INSTANCE_ID=$(aws ec2 describe-instances \
                --filters "Name=tag:Name,Values=$INSTANCE_NAME" "Name=instance-state-name,Values=running,stopped,stopping,pending" \
                --query 'Reservations[*].Instances[*].[InstanceId,LaunchTime]' \
                --output text 2>/dev/null | sort -k2 -r | head -1 | awk '{print $1}' || echo "")
            fi
            
            if [ -n "$INSTANCE_ID" ] && [ "$INSTANCE_ID" != "None" ]; then
              # Check if there's a volume attached to this instance
              VOL_ID=$(aws ec2 describe-volumes \
                --filters "Name=attachment.instance-id,Values=$INSTANCE_ID" "Name=tag:Name,Values=terraform-state-storage" \
                --query 'Volumes[0].VolumeId' \
                --output text 2>/dev/null || echo "")
              
              if [ -n "$VOL_ID" ] && [ "$VOL_ID" != "None" ]; then
                echo "Found attached volume: $VOL_ID on instance: $INSTANCE_ID"
                echo "âš ï¸  Volume attachment not in state - will detach manually before destroy"
                echo "This ensures the volume can be destroyed without being stuck"
                
                # Detach the volume (force detach to avoid hanging)
                aws ec2 detach-volume \
                  --volume-id "$VOL_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --force 2>/dev/null || echo "Volume may already be detached or detaching"
                
                # Wait for detachment to complete
                echo "Waiting for volume to detach..."
                aws ec2 wait volume-available --volume-ids "$VOL_ID" --timeout 60 || echo "Volume detachment may still be in progress"
              else
                echo "No attached volume found - proceeding normally"
              fi
            else
              echo "No instance found - proceeding normally"
            fi
          fi
        continue-on-error: true

      - name: Terraform Plan Destroy
        working-directory: infra/terraform
        run: |
          echo "ğŸ“‹ Generating destroy plan..."
          terraform plan -destroy -var-file=${{ steps.use-tfvars.outputs.tfvars_file }} -var="key_pair_name=${{ secrets.TERRAFORM_KEY_PAIR_NAME }}" -out=destroy.tfplan
          echo ""
          echo "âš ï¸ DESTRUCTION PLAN SUMMARY:"
          terraform show -no-color destroy.tfplan | head -100
        continue-on-error: true

      - name: Terraform Destroy
        working-directory: infra/terraform
        run: |
          echo "ğŸ”¥ Starting infrastructure destruction..."
          echo "This will destroy:"
          echo "  - EC2 instance (and all containers/services on it)"
          echo "  - Security groups"
          echo "  - EBS volumes (including Terraform state volume)"
          echo "  - Volume attachments"
          echo ""
          terraform destroy -auto-approve -var-file=${{ steps.use-tfvars.outputs.tfvars_file }} -var="key_pair_name=${{ secrets.TERRAFORM_KEY_PAIR_NAME }}"
        continue-on-error: true

      - name: Verify Destruction
        working-directory: infra/terraform
        run: |
          echo "ğŸ” Verifying all resources are destroyed..."
          terraform show 2>/dev/null || echo "âœ… State file is empty or invalid (expected after destruction)"
          
          # Try to list any remaining resources
          echo ""
          echo "Checking for orphaned resources..."
          
          # Check for EC2 instances (environment-specific)
          ENV="${{ env.ENVIRONMENT }}"
          INSTANCE_NAME="todo-app-server-${ENV}"
          INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=$INSTANCE_NAME" "Name=instance-state-name,Values=running,stopped,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "âš ï¸ WARNING: Found EC2 instances: $INSTANCES"
            echo "They may still be terminating. Check AWS Console."
          else
            echo "âœ… No EC2 instances found"
          fi
          
          # Check for security groups (environment-specific)
          SG_NAME="todo-app-sg-${ENV}"
          SGS=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[*].GroupId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$SGS" ] && [ "$SGS" != "None" ]; then
            echo "âš ï¸ WARNING: Found security groups: $SGS"
            echo "Attempting to delete..."
            for sg in $SGS; do
              aws ec2 delete-security-group --group-id "$sg" 2>/dev/null || echo "Could not delete $sg (may have dependencies)"
            done
          else
            echo "âœ… No security groups found"
          fi
          
          # Check for EBS volumes
          VOLUMES=$(aws ec2 describe-volumes \
            --filters "Name=tag:Name,Values=terraform-state-storage" \
            --query 'Volumes[*].VolumeId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$VOLUMES" ] && [ "$VOLUMES" != "None" ]; then
            echo "âš ï¸ WARNING: Found EBS volumes: $VOLUMES"
            echo "Attempting to delete..."
            for vol in $VOLUMES; do
              aws ec2 delete-volume --volume-id "$vol" 2>/dev/null || echo "Could not delete $vol"
            done
          else
            echo "âœ… No EBS volumes found"
          fi

      - name: Clean up local state files
        working-directory: infra/terraform
        run: |
          echo "ğŸ§¹ Cleaning up local state files..."
          rm -f terraform.tfstate terraform.tfstate.backup destroy.tfplan *.tfvars
          echo "âœ… Local state files removed"

      - name: Delete state artifact
        run: |
          echo "ğŸ§¹ Note: Terraform state artifact will be automatically cleaned up after workflow completion"
          echo "GitHub Actions artifacts are retained for 90 days but can be manually deleted"

      - name: Destruction Summary
        if: always()
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ—‘ï¸  INFRASTRUCTURE DESTRUCTION COMPLETE"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "âœ… Destroyed resources:"
          echo "   - EC2 instance and all services"
          echo "   - Security groups"
          echo "   - EBS volumes"
          echo "   - All containers and applications"
          echo ""
          echo "ğŸ“ Next steps:"
          echo "   1. Verify in AWS Console that all resources are gone"
          echo "   2. Delete any orphaned resources manually if needed"
          echo "   3. Clean up GitHub Actions artifacts (optional)"
          echo "   4. All costs should now be $0.00"
          echo ""
          echo "âš ï¸  IMPORTANT:"
          echo "   - State file has been destroyed"
          echo "   - To recreate infrastructure, run the infrastructure workflow again"
          echo "   - All data on the instance is permanently lost"
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

